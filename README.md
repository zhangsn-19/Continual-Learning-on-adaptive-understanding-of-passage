# Continual-Learning-on-adaptive-understanding-of-passage

## Extensive Related work Research

### Adaptive learning task focused on continual learning

### Logical understanding of the passage or the sentence

## Paper List

### Continual Learning in Grid Settings

##### Continual Learning with Deep Generative Replay

**[Paper]** : https://arxiv.org/pdf/1705.08690.pdf

##### Three scenarios for continual learning

**[Paper]** : https://arxiv.org/pdf/1904.07734.pdf

**[Repo]** : https://github.com/GMvandeVen/continual-learning

Classified and given three different settings / definitions:

Task-incremental learning & Domain-incremental learning & Class-incremental learning

##### Gradient Episodic Memory for Continual Learning

**[Paper]** : https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf

##### Variational Continual Learning

**[Paper]** : https://arxiv.org/pdf/1710.10628.pdf

**[Repo]** : https://github.com/nvcuong/variational-continual-learning

##### Task Free Continual Learning

**[Paper]** : https://openaccess.thecvf.com/content_CVPR_2019/papers/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.pdf

##### Progress & Compress: A scalable framework for continual learning

**[Paper]** : http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf

##### Meta-learning Representations for continual learning

**[Paper]** : https://arxiv.org/pdf/1905.12588.pdf

Three methods to prevent catastrophic forgetting basically: (1) modify the online update to retain knowledge, (2) replay or generate samples for more updates and (3) use semi-distributed representations.

![image-20211020130121544](C:\Users\Think\AppData\Roaming\Typora\typora-user-images\image-20211020130121544.png)

##### Meta-Learning Representations for continual learning

**[Paper]** : https://arxiv.org/pdf/1905.12588.pdf

**[Repo]** : https://github.com/khurramjaved96/mrcl

##### Learning without forgetting

**[Paper]** : https://export.arxiv.org/pdf/1606.09282

##### Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence

**[Paper]** : https://openaccess.thecvf.com/content_ECCV_2018/papers/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.pdf

 

#### Continual Learning in Graph Neural Networks

##### Overcoming Catastrophic Forgetting in Graph Neural Networks

**[Paper]** : https://arxiv.org/pdf/2012.06002.pdf

##### Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay

**[Paper]** : https://www.aaai.org/AAAI21Papers/AAAI-4967.ZhouF.pdf

##### Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting

**[Paper]** : https://arxiv.org/pdf/1802.02950.pdf

##### Learning To Learn Without Forgetting By Maximizing Transfer And Minimizing Interference

**[Paper]** : https://arxiv.org/pdf/1810.11910.pdf



#### Other miscellaneous including continual learning or knowledge distillation

##### Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data

**[Paper]** : https://arxiv.org/pdf/2009.09427.pdf

Including some of the knowledge distillation methods to pair dialogues and expand dataset size

##### Continual Learning for Natural Language Generation in Task-oriented Dialog Systems

**[Paper]** : https://arxiv.org/pdf/2010.00910.pdf

**[Repo]** : https://github.com/MiFei/Continual-Learning-for-NLG/blob/master/model/cvae.py

Seemingly utilized  and implemented using CVAE

##### Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems

**[Paper]** : https://arxiv.org/pdf/1905.05644.pdf

domain adaption work



###### By Shuning Zhang and Zhen Zhang, CopyRight 2021