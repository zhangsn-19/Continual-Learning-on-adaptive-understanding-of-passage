## 10.20 Report

和黄民烈老师讨论的过程中收获了一些想法。

### Continual Learning 前景

- Continual Learning很重要，在很多场景下都应该也值得研究，但是continual learning当前的研究很多，而且在2020年和2021年创新性研究在逐渐减少。
- Continual Learning主要分为三个方面：regularization based, replay based和...（在知乎那篇文章上有），这三个方面都比较extensive researched。

### Continual Learning 研究问题

- 虽然从某种程度上 CL 研究很多，但是在以下任务上还有很大探索空间：

  - Task 级别的 Continual Learning (Multi-Tasking)

    虽然 Multi-Tasking很多，但是有质量的Multi-Tasking和很多其中的原理性的问题没有解决，解释性也极差。

    黄老师举了一个例子，就是大模型中不同的 Task 在 learn in a sequence 的时候有些时候后面的 Task 学习过程中前面的 Task 会有 Catastrophic forgetting 的问题，有的没有，这个的原因可能是 Task 在隐层映射到的参数空间的位置不同，那么怎么调控这个参数空间的分布，怎么去可视化和研究这个问题，由此解决 Catastrophic forgetting 的问题是一个可探索的方向。

    这个过程中可能有些 Task 很相似 （例如 情感分析中的分类和文本、句子的成分分类等等），相似的 Task 会不会造成更强的 Catastrophic forgetting，其中的原理是什么？

  - Data 和 Domain 级别的 Continual Learning ( Data Transfer Learning)

    另外一个例子就是在 Dialogue Tracking 的 Continual Learning 过程中，黄老师的学生发现在有些 Data 分布下就会有学习后面的 Task 的时候前面的 Task Catastrophic forgetting的问题，有些 Data 分布下就不会，可能这和数据集 不同 Domain 在参数空间上的映射有关系，是个很值得研究的问题。

- 在以下领域也是可以做的：

  - 是否可以去 Define 在一些特定场景下经过一些特定模型结构 Topological 的调整能够让某些任务结果更优 ？
  - 是否可以研究不同模型大小和不同 Structure 对 Catastrophic forgetting 量化和定性化的影响？